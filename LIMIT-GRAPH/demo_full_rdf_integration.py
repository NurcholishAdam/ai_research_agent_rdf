#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Complete RDF Integration Demo for LIMIT-Graph
Demonstrates all five RDF components working together:
1. Triple Generator Scaffold
2. Named Graphs for Provenance  
3. Ontology Templates
4. SPARQL-Compatible Views
5. Multilingual Support with Cultural Context
"""

import sys
import os
from datetime import datetime
import json

# Add project paths
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))

# Import all RDF components
from extensions.LIMIT_GRAPH.rdf import create_full_rdf_system
from extensions.LIMIT_GRAPH.rdf.named_graphs import (
    AnnotatorProfile, LanguageAnnotation, RLHFFeedbackTrace
)
from extensions.LIMIT_GRAPH.rdf.ontology_templates import OntologyClass, OntologyProperty

def demo_complete_rdf_integration():
    """Demonstrate complete RDF integration with all components"""
    
    print("ðŸš€ LIMIT-Graph Complete RDF Integration Demo")
    print("=" * 60)
    
    # Step 1: Initialize Complete RDF System
    print("\nðŸ“‹ Step 1: Initializing Complete RDF System")
    print("-" * 40)
    
    rdf_system = create_full_rdf_system("http://demo.limitgraph.org/")
    
    print("âœ… Triple Generator: Ready")
    print("âœ… Named Graph Manager: Ready") 
    print("âœ… Ontology Builder: Ready")
    print("âœ… SPARQL Interface: Ready")
    
    # Step 2: Create Multilingual Corpus
    print("\nðŸŒ Step 2: Processing Multilingual Corpus")
    print("-" * 40)
    
    multilingual_corpus = [
        # Spanish examples
        {"_id": "es_001", "text": "A AndrÃ©s le gustan las manzanas rojas."},
        {"_id": "es_002", "text": "MarÃ­a vive en Barcelona y estudia lingÃ¼Ã­stica."},
        {"_id": "es_003", "text": "Los niÃ±os juegan en el parque cerca de la escuela."},
        
        # Arabic examples  
        {"_id": "ar_001", "text": "Ø£Ø­Ù…Ø¯ ÙŠØ­Ø¨ Ø§Ù„ØªÙØ§Ø­ Ø§Ù„Ø£Ø­Ù…Ø±."},
        {"_id": "ar_002", "text": "ÙØ§Ø·Ù…Ø© ØªØ¹ÙŠØ´ ÙÙŠ Ø§Ù„Ù‚Ø§Ù‡Ø±Ø© ÙˆØªØ¯Ø±Ø³ Ø§Ù„Ø·Ø¨."},
        {"_id": "ar_003", "text": "Ø§Ù„Ø£Ø·ÙØ§Ù„ ÙŠÙ„Ø¹Ø¨ÙˆÙ† ÙÙŠ Ø§Ù„Ø­Ø¯ÙŠÙ‚Ø©."},
        
        # Indonesian examples
        {"_id": "id_001", "text": "Ahmad suka apel merah."},
        {"_id": "id_002", "text": "Sari tinggal di Jakarta dan belajar komputer."},
        {"_id": "id_003", "text": "Anak-anak bermain di taman."},
        
        # English examples
        {"_id": "en_001", "text": "Alice likes red apples."},
        {"_id": "en_002", "text": "Bob lives in New York and studies engineering."},
        {"_id": "en_003", "text": "Children play in the park near the school."}
    ]
    
    # Convert corpus to RDF triples
    generator = rdf_system['triple_generator']
    conversion_result = generator.convert_limitgraph_corpus(
        multilingual_corpus,
        annotator_id="multilingual_demo_expert"
    )
    
    print(f"ðŸ“Š Conversion Results:")
    print(f"  - Triples generated: {conversion_result.triples_count}")
    print(f"  - Languages detected: {conversion_result.languages_detected}")
    print(f"  - Documents processed: {len(multilingual_corpus)}")
    
    # Step 3: Register Multilingual Annotators
    print("\nðŸ‘¥ Step 3: Registering Multilingual Annotators")
    print("-" * 40)
    
    manager = rdf_system['named_graph_manager']
    
    annotators = [
        AnnotatorProfile(
            annotator_id="ann_es_001",
            name="Dr. MarÃ­a GonzÃ¡lez",
            expertise_domains=["spanish_nlp", "linguistics", "cultural_studies"],
            languages=["es", "en"],
            reliability_score=0.96,
            annotation_count=1500,
            created_date=datetime.now()
        ),
        AnnotatorProfile(
            annotator_id="ann_ar_001", 
            name="Ø¯. Ø£Ø­Ù…Ø¯ Ø­Ø³Ù†",  # Dr. Ahmed Hassan in Arabic
            expertise_domains=["arabic_nlp", "middle_eastern_studies"],
            languages=["ar", "en"],
            reliability_score=0.94,
            annotation_count=1200,
            created_date=datetime.now()
        ),
        AnnotatorProfile(
            annotator_id="ann_id_001",
            name="Dr. Sari Wijaya",
            expertise_domains=["indonesian_nlp", "southeast_asian_studies"],
            languages=["id", "en"],
            reliability_score=0.93,
            annotation_count=980,
            created_date=datetime.now()
        )
    ]
    
    for annotator in annotators:
        manager.register_annotator(annotator)
        print(f"  âœ… Registered: {annotator.name} ({', '.join(annotator.languages)})")
    
    # Step 4: Add Cultural Language Annotations
    print("\nðŸ·ï¸ Step 4: Adding Cultural Language Annotations")
    print("-" * 40)
    
    from rdflib import URIRef
    
    cultural_annotations = [
        {
            'statement_uri': URIRef("http://demo.limitgraph.org/data/statement_es_001"),
            'annotation': LanguageAnnotation(
                text="A AndrÃ©s le gustan las manzanas rojas",
                language_code="es",
                confidence=0.98,
                detection_method="native_speaker_validation",
                cultural_context="iberian_spanish",
                dialect_variant="peninsular_spanish"
            )
        },
        {
            'statement_uri': URIRef("http://demo.limitgraph.org/data/statement_ar_001"),
            'annotation': LanguageAnnotation(
                text="Ø£Ø­Ù…Ø¯ ÙŠØ­Ø¨ Ø§Ù„ØªÙØ§Ø­ Ø§Ù„Ø£Ø­Ù…Ø±",
                language_code="ar",
                confidence=0.97,
                detection_method="arabic_script_detection",
                cultural_context="levantine_arabic",
                dialect_variant="modern_standard_arabic"
            )
        },
        {
            'statement_uri': URIRef("http://demo.limitgraph.org/data/statement_id_001"),
            'annotation': LanguageAnnotation(
                text="Ahmad suka apel merah",
                language_code="id",
                confidence=0.95,
                detection_method="indonesian_pattern_matching",
                cultural_context="javanese_indonesian",
                dialect_variant="jakarta_indonesian"
            )
        }
    ]
    
    for item in cultural_annotations:
        lang_id = manager.add_language_annotation(item['statement_uri'], item['annotation'])
        print(f"  âœ… Added {item['annotation'].language_code} annotation: {lang_id}")
    
    # Step 5: Add RLHF Feedback with Cultural Appropriateness
    print("\nðŸŽ¯ Step 5: Adding RLHF Feedback with Cultural Context")
    print("-" * 40)
    
    rlhf_feedbacks = [
        RLHFFeedbackTrace(
            feedback_id="fb_es_001",
            statement_id="statement_es_001",
            feedback_type="positive",
            quality_score=0.94,
            relevance_score=0.91,
            cultural_appropriateness=0.97,
            feedback_text="Excellent Spanish extraction with proper cultural context",
            annotator_id="ann_es_001",
            timestamp=datetime.now(),
            session_id="demo_session_001"
        ),
        RLHFFeedbackTrace(
            feedback_id="fb_ar_001",
            statement_id="statement_ar_001", 
            feedback_type="positive",
            quality_score=0.92,
            relevance_score=0.89,
            cultural_appropriateness=0.95,
            feedback_text="Good Arabic extraction, culturally appropriate",
            annotator_id="ann_ar_001",
            timestamp=datetime.now(),
            session_id="demo_session_002"
        ),
        RLHFFeedbackTrace(
            feedback_id="fb_id_001",
            statement_id="statement_id_001",
            feedback_type="positive", 
            quality_score=0.90,
            relevance_score=0.87,
            cultural_appropriateness=0.93,
            feedback_text="Accurate Indonesian extraction with Jakarta dialect recognition",
            annotator_id="ann_id_001",
            timestamp=datetime.now(),
            session_id="demo_session_003"
        )
    ]
    
    for feedback in rlhf_feedbacks:
        feedback_id = manager.add_rlhf_feedback_trace(feedback)
        print(f"  âœ… Added {feedback.feedback_type} feedback: {feedback_id}")
        print(f"    Quality: {feedback.quality_score:.2f}, Cultural: {feedback.cultural_appropriateness:.2f}")
    
    # Step 6: Create Domain-Specific Ontology
    print("\nðŸ—ï¸ Step 6: Creating Multilingual Food Domain Ontology")
    print("-" * 40)
    
    builder = rdf_system['ontology_builder']
    
    # Create food domain classes
    food_classes = [
        OntologyClass(
            class_uri=str(builder.LGONT.Food),
            labels={
                'en': 'Food',
                'es': 'Comida', 
                'ar': 'Ø·Ø¹Ø§Ù…',
                'id': 'Makanan'
            },
            alt_labels={
                'en': ['Edible', 'Nourishment', 'Sustenance'],
                'es': ['Alimento', 'Comestible', 'Sustento'],
                'ar': ['ØºØ°Ø§Ø¡', 'Ø£ÙƒÙ„', 'Ù‚ÙˆØª'],
                'id': ['Pangan', 'Santapan', 'Hidangan']
            },
            description={
                'en': 'Edible substance that provides nutrition',
                'es': 'Sustancia comestible que proporciona nutriciÃ³n',
                'ar': 'Ù…Ø§Ø¯Ø© ØµØ§Ù„Ø­Ø© Ù„Ù„Ø£ÙƒÙ„ ØªÙˆÙØ± Ø§Ù„ØªØºØ°ÙŠØ©',
                'id': 'Zat yang dapat dimakan dan memberikan nutrisi'
            },
            parent_classes=[str(builder.LGONT.Entity)],
            properties=['hasNutrient', 'hasColor', 'hasOrigin']
        ),
        OntologyClass(
            class_uri=str(builder.LGONT.Fruit),
            labels={
                'en': 'Fruit',
                'es': 'Fruta',
                'ar': 'ÙØ§ÙƒÙ‡Ø©', 
                'id': 'Buah'
            },
            alt_labels={
                'en': ['Fresh fruit', 'Tree fruit'],
                'es': ['Fruta fresca', 'Fruto'],
                'ar': ['ÙØ§ÙƒÙ‡Ø© Ø·Ø§Ø²Ø¬Ø©', 'Ø«Ù…Ø±Ø©'],
                'id': ['Buah segar', 'Buah-buahan']
            },
            description={
                'en': 'Sweet and fleshy product of a tree or plant',
                'es': 'Producto dulce y carnoso de un Ã¡rbol o planta',
                'ar': 'Ù…Ù†ØªØ¬ Ø­Ù„Ùˆ ÙˆÙ„Ø­Ù…ÙŠ Ù…Ù† Ø´Ø¬Ø±Ø© Ø£Ùˆ Ù†Ø¨Ø§Øª',
                'id': 'Produk manis dan berdaging dari pohon atau tanaman'
            },
            parent_classes=[str(builder.LGONT.Food)],
            properties=['hasColor', 'hasTaste', 'hasVitamin']
        )
    ]
    
    # Create food domain properties
    food_properties = [
        OntologyProperty(
            property_uri=str(builder.LGONT.hasColor),
            property_type='ObjectProperty',
            labels={
                'en': 'has color',
                'es': 'tiene color',
                'ar': 'Ù„Ù‡ Ù„ÙˆÙ†',
                'id': 'memiliki warna'
            },
            alt_labels={
                'en': ['colored as', 'appears as'],
                'es': ['coloreado como', 'aparece como'],
                'ar': ['Ù…Ù„ÙˆÙ† ÙƒÙ€', 'ÙŠØ¸Ù‡Ø± ÙƒÙ€'],
                'id': ['berwarna', 'tampak sebagai']
            },
            description={
                'en': 'Indicates the color property of an object',
                'es': 'Indica la propiedad de color de un objeto',
                'ar': 'ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø®Ø§ØµÙŠØ© Ù„ÙˆÙ† Ø§Ù„ÙƒØ§Ø¦Ù†',
                'id': 'Menunjukkan properti warna suatu objek'
            },
            domain=str(builder.LGONT.Entity),
            range=str(builder.LGONT.Entity),
            parent_properties=[]
        )
    ]
    
    builder.create_domain_specific_ontology("food", food_classes, food_properties)
    
    # Get ontology statistics
    onto_stats = builder.get_ontology_statistics()
    print(f"ðŸ“Š Ontology Statistics:")
    print(f"  - Classes: {onto_stats['classes']}")
    print(f"  - Object Properties: {onto_stats['object_properties']}")
    print(f"  - Languages: {onto_stats['languages_supported']}")
    print(f"  - Total Triples: {onto_stats['total_triples']}")
    
    # Step 7: Demonstrate SPARQL Queries
    print("\nðŸ” Step 7: Demonstrating SPARQL Multilingual Queries")
    print("-" * 40)
    
    interface = rdf_system['sparql_interface']
    
    # Query 1: Find all Spanish content
    print("Query 1: Spanish content search")
    es_results = interface.multilingual_search(
        search_term="manzanas",
        languages=['es'],
        min_confidence=0.8
    )
    print(f"  Results: {es_results.result_count} Spanish matches")
    
    # Query 2: Cross-cultural apple concept
    print("\nQuery 2: Cross-cultural 'apple' concept")
    apple_results = interface.cross_cultural_analysis(
        concept_term="apple",
        languages=['en', 'es', 'ar', 'id']
    )
    print(f"  Results: {apple_results.result_count} cross-cultural matches")
    
    # Query 3: RLHF feedback analysis
    print("\nQuery 3: RLHF feedback analysis")
    feedback_results = interface.analyze_rlhf_feedback(
        feedback_type="positive",
        min_quality=0.9
    )
    print(f"  Results: {feedback_results.result_count} high-quality feedback entries")
    
    # Query 4: Available languages
    print("\nQuery 4: Available languages in dataset")
    languages = interface.get_available_languages()
    for lang in languages:
        print(f"  - {lang['name']} ({lang['code']}): {lang['annotation_count']} annotations")
    
    # Step 8: Export Complete RDF Dataset
    print("\nðŸ’¾ Step 8: Exporting Complete RDF Dataset")
    print("-" * 40)
    
    # Export triples in multiple formats
    triple_files = generator.export_to_formats("output/rdf/demo/")
    print("Triple files exported:")
    for format_name, filepath in triple_files.items():
        print(f"  âœ… {format_name}: {filepath}")
    
    # Export named graphs
    named_graph_files = manager.export_named_graphs("output/rdf/demo/named_graphs/")
    print("\nNamed graph files exported:")
    for graph_name, formats in named_graph_files.items():
        print(f"  ðŸ“Š {graph_name}: {len(formats)} formats")
    
    # Export ontology
    ontology_files = builder.export_ontology("output/rdf/demo/ontologies/")
    print("\nOntology files exported:")
    for format_name, filepath in ontology_files.items():
        print(f"  ðŸ—ï¸ {format_name}: {filepath}")
    
    # Step 9: Generate Summary Report
    print("\nðŸ“‹ Step 9: Generating Summary Report")
    print("-" * 40)
    
    # Collect all statistics
    triple_stats = generator.get_statistics()
    named_graph_stats = manager.get_statistics()
    
    summary_report = {
        "demo_timestamp": datetime.now().isoformat(),
        "corpus_processed": {
            "documents": len(multilingual_corpus),
            "languages": conversion_result.languages_detected,
            "triples_generated": conversion_result.triples_count
        },
        "annotators_registered": len(annotators),
        "language_annotations": len(cultural_annotations),
        "rlhf_feedback_entries": len(rlhf_feedbacks),
        "ontology_statistics": onto_stats,
        "named_graph_statistics": named_graph_stats,
        "sparql_queries_executed": 4,
        "exported_formats": {
            "triple_formats": list(triple_files.keys()),
            "named_graph_formats": len(named_graph_files),
            "ontology_formats": list(ontology_files.keys())
        }
    }
    
    # Save summary report
    os.makedirs("output/rdf/demo/", exist_ok=True)
    with open("output/rdf/demo/summary_report.json", "w", encoding="utf-8") as f:
        json.dump(summary_report, f, indent=2, ensure_ascii=False)
    
    print("ðŸ“Š Summary Report:")
    print(f"  - Documents processed: {summary_report['corpus_processed']['documents']}")
    print(f"  - Languages detected: {len(summary_report['corpus_processed']['languages'])}")
    print(f"  - Triples generated: {summary_report['corpus_processed']['triples_generated']}")
    print(f"  - Annotators registered: {summary_report['annotators_registered']}")
    print(f"  - RLHF feedback entries: {summary_report['rlhf_feedback_entries']}")
    print(f"  - Export formats: {len(summary_report['exported_formats']['triple_formats'])}")
    
    # Step 10: Final Integration Assessment
    print("\nðŸŽ¯ Step 10: Final Integration Assessment")
    print("-" * 40)
    
    assessment_score = 0
    max_score = 10
    
    # Check each component
    components_status = {
        "Triple Generation": conversion_result.triples_count > 0,
        "Named Graphs": named_graph_stats['overall']['total_triples'] > 0,
        "Ontology Creation": onto_stats['total_triples'] > 0,
        "SPARQL Interface": len(languages) > 0,
        "Multilingual Support": len(conversion_result.languages_detected) >= 3,
        "Cultural Context": len(cultural_annotations) > 0,
        "RLHF Integration": len(rlhf_feedbacks) > 0,
        "Provenance Tracking": len(annotators) > 0,
        "Export Functionality": len(triple_files) > 0,
        "Cross-Cultural Analysis": apple_results.result_count >= 0
    }
    
    for component, status in components_status.items():
        if status:
            assessment_score += 1
            print(f"  âœ… {component}: PASS")
        else:
            print(f"  âŒ {component}: FAIL")
    
    success_rate = (assessment_score / max_score) * 100
    
    print(f"\nðŸ† FINAL ASSESSMENT:")
    print(f"  Score: {assessment_score}/{max_score} ({success_rate:.1f}%)")
    
    if success_rate >= 90:
        print("  ðŸŽ‰ EXCELLENT: Complete RDF integration successful!")
        print("  ðŸ’¡ System ready for production deployment")
    elif success_rate >= 75:
        print("  âœ… GOOD: RDF integration mostly successful")
        print("  ðŸ’¡ Minor improvements needed")
    elif success_rate >= 50:
        print("  âš ï¸ FAIR: RDF integration partially successful")
        print("  ðŸ’¡ Several components need attention")
    else:
        print("  âŒ POOR: RDF integration needs significant work")
        print("  ðŸ’¡ Major components failing")
    
    print("\nðŸŒŸ RDF INTEGRATION FEATURES DEMONSTRATED:")
    print("  âœ… Triple generation from LIMIT-Graph edges")
    print("  âœ… Named graphs for provenance tracking")
    print("  âœ… Multilingual ontology templates")
    print("  âœ… SPARQL query interface")
    print("  âœ… Cultural context awareness")
    print("  âœ… RLHF feedback integration")
    print("  âœ… Cross-cultural analysis")
    print("  âœ… Multi-format export")
    print("  âœ… Ethical audit trails")
    print("  âœ… Contributor-friendly architecture")
    
    print("\nðŸš€ NEXT STEPS:")
    print("  1. Review generated RDF files in output/rdf/demo/")
    print("  2. Validate RDF with SHACL/OWL tools")
    print("  3. Contribute multilingual ontology terms")
    print("  4. Add more cultural contexts")
    print("  5. Expand SPARQL query templates")
    
    return success_rate >= 75

if __name__ == "__main__":
    success = demo_complete_rdf_integration()
    print(f"\n{'='*60}")
    print(f"Demo completed {'successfully' if success else 'with issues'}")
    sys.exit(0 if success else 1)